{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "191bd1df",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Methods](#methods)\n",
    "\n",
    "   2.1. [Preprocessing](#preprocessing)\n",
    "      * [Terminology & examples](#some-terminology)\n",
    "\n",
    "   2.2. [Extracting shared constructions](#extracting-shared-constructions)\n",
    "\n",
    "   2.3. [Filtering shared constructions](#linking-shared-constructions)\n",
    "\n",
    "   2.4. [Linking shared constructions](#linking-shared-constructions)\n",
    "      * [Example](#example)\n",
    "\n",
    "3. [Analysis & results](#results)\n",
    "\n",
    "   3.1. [Presence of Shared Constructions and their Patterns of Use over Dialogue Rounds](#presence_alignment)\n",
    "   \n",
    "   3.2. [Individual Speaker Names versus Interactive Shared Constructions](#individual_vs_shared) \n",
    "        \n",
    "   3.3. [Naming Convergence Across Speakers is Linked to Shared Constructions](#naming_convergence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16efc3",
   "metadata": {},
   "source": [
    "### Note: please note that, in this notebook, we use the term \"construction\" to refer to a sequence of lemmas that is repeated across multiple dialogues. This term is used interchangeably with \"expression\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0234a386",
   "metadata": {},
   "source": [
    "### Import libraries & methods <a class=\"anchor\" id=\"importing_libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f73c478b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code to import python libraries and set up the notebook\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import colorcet as cc\n",
    "\n",
    "from utils.read_labeled_and_pos_target_data import assert_match_betwee_shared_exp_and_actual_utterances,prepare_dialogue_shared_expressions_and_turns_info, extract_all_shared_exp_info, all_func_words\n",
    "#from utils.link_target_expressions import link_shared_expressions_final_algo, link_identical_shared_expressions_based_on_pair, link_shared_expressions_fribble_based\n",
    "#from utils.data_containers_forced_alignment import Turn, Gesture, Utterance\n",
    "from utils.read_stop_function_words import read_stop_function_words\n",
    "\n",
    "import spacy\n",
    "from utils.similarity_measures import get_naming_task\n",
    "# use genism to load the word2vec model\n",
    "\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "palette = sns.color_palette(cc.glasbey_hv, n_colors=25)\n",
    "sns.set_theme(style=\"whitegrid\", palette=palette) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b68148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_labeled_and_pos_target_data_pseudo import prepare_dialogue_shared_expressions_and_turns_info_pseudo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15dfe0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ef1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#space_lg_nlp = spacy.load(\"nl_core_news_lg\")\n",
    "\n",
    "space_lg_nlp = spacy.load(\"en_core_web_lg\")\n",
    "naming_task = get_naming_task(nlp=space_lg_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875a46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to set up the paths\n",
    "dialign_output = '../dialign/output_targets_riws_lemma_lg/'\n",
    "turn_info_path = '../dialign/targets_riws_lemma_lg/'\n",
    "fribbles_path = \"../data/CABB/objects_fribbles/{}.jpg\"\n",
    "videos_path = '~/data/{}_synced_overview.mp4'\n",
    "pos_func_words = ['DET', 'PRON', 'ADP', 'CCONJ', 'SCONJ', 'AUX', 'PART', 'PUNCT', 'SYM', 'X', 'INTJ', 'NUM', 'SPACE']\n",
    "frequent_words, function_words = read_stop_function_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "290ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to set up the paths\n",
    "pseudo_dialign_output = '../dialign/output_targets_riws_lemma_pseudo'\n",
    "pseudo_turn_info_path = '../dialign/targets_riws_lemma_pseudo/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13db29b1",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "<!-- # ~/anaconda3/envs/lex/bin/pip install statannotations\n",
    "# /Users/esamghaleb/anaconda3/bin/pip install spacy -->\n",
    "This notebook includes the data processing methods used to operationalise linguistic alignment and the results reported in the paper. Linguistic alignment is detected based on a lemmatized speech at the level of the lemmas' form. In a Pairs, single lemmas or sequences of lemmas used at least once by **both** participants are defined as shared expressions and considered instances of lexical linguistic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eb20383",
   "metadata": {},
   "source": [
    "# 2. Methods<a class=\"anchor\" id=\"methods\"></a>\n",
    "\n",
    "# 2.1. Pre-processing Dialogues and Extracting Shared Constructions <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "The details of the pre-processing steps are described README.md file in the main directory of the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6680cdfb",
   "metadata": {},
   "source": [
    "## Some terminology <a class=\"anchor\" id=\"some-terminology\"></a>\n",
    "\n",
    "* **Initiation turn/round:** The turn or round where a shared expression was first introduced by one of the participants.\n",
    "* **Establishing turn/round:** The turn or round where the shared expression was first repeated by the other participant, i.e., it becomes shared.\n",
    "* **Speaker role:** **D** & **M** indicate whether the shared expression was used by the director or the matcher.\n",
    "* **Initiator role:** The participant who introduced the shared expression, i.e., the director or the matcher.\n",
    "* **Establisher role:** The participant who established the shared expression, i.e., the director or the matcher.\n",
    "* **Turns** and **rounds:** the turns and rounds numbers where shared expressions are used. \n",
    "* **Length:** the number of lemmas in the shared expression.\n",
    "* **Target fribble:** the fribble for which the shared expression was used.\n",
    "* **Frequency:** the number of times the shared expression was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "058a58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_func_words = ['DET', 'PRON', 'ADP', 'CCONJ', 'SCONJ', 'AUX', 'PART', 'PUNCT', 'SYM', 'X', 'INTJ', 'SPACE', 'NUM']\n",
    "stop_words, function_words = read_stop_function_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cc6a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.2. Extracting shared constructions <a class=\"anchor\" id = \"extracting-shared-constructions\"></a>\n",
    "\n",
    "The steps to extract shared constructions are explained in the README.md file in the main directory of the project. In this notebook, we load the extracted shared constructions and analyse them. We load the shared constructions for real and control dialogues and compare them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af62695",
   "metadata": {},
   "source": [
    "### Load shared constructions of real dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31971130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to read the shared expressions and turns info of real pairs\n",
    "real_shared_constructions_info, real_turns_info = prepare_dialogue_shared_expressions_and_turns_info(dialign_output, turn_info_path)\n",
    "real_exp_info = extract_all_shared_exp_info(real_shared_constructions_info, real_turns_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236499b2",
   "metadata": {},
   "source": [
    "### Load shared constructions of Pseudo Pairs (control dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072251ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pseudo_shared_constructions_info, pseudo_turns_info = prepare_dialogue_shared_expressions_and_turns_info_pseudo(pseudo_dialign_output, pseudo_turn_info_path)\n",
    "#pseudo_exp_info = extract_all_shared_exp_info(pseudo_shared_constructions_info, pseudo_turns_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f938d",
   "metadata": {},
   "source": [
    "# 2.3. Filtering shared constructions <a class=\"anchor\" id=\"filtering-shared-constructions\"></a>\n",
    "We filter out shared constructions consisting only of function words or very common words. To identify function words, we use POS tagging and remove phrases that only contain them. We also consider highly frequent words based on word frequencies in the SUBTLEX-NL corpus. We focus on shared constructions used by one fribble in the entire dialogue and set aside those used for multiple fribbles for future analysis. This gives us a set of unique shared constructions per fribble, representing the contentful lemmas used exclusively by both participants for each fribble.\n",
    "\n",
    "More details on the filtering process are provided in the README.md file in the main directory of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05527592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_exp_info(exp_info, function_words, stop_words):\n",
    "   exp_with_only_func_words = np.logical_not([all_func_words(pos_seq, pos_func_words) for idx, pos_seq in enumerate(exp_info['pos_seq'].to_list())])\n",
    "   exp_info = exp_info[exp_with_only_func_words]\n",
    "   exp_with_only_func_words = np.logical_not([all_func_words(exp, function_words) for idx, exp in enumerate(exp_info['exp'].to_list())])\n",
    "   exp_info = exp_info[exp_with_only_func_words]\n",
    "   exp_with_only_stop_words = np.logical_not([all_func_words(exp, stop_words) for idx, exp in enumerate(exp_info['exp'].to_list())])\n",
    "   exp_info = exp_info[exp_with_only_stop_words]\n",
    "   return exp_info\n",
    "def link_exp_info(exp_info, turns_info, shared_constructions_info):\n",
    "   exp_info['shared expressions'] = exp_info['exp'].apply(lambda x: [x])\n",
    "   exp_info = link_identical_shared_expressions_based_on_pair(turns_info, exp_info)\n",
    "   # check if there are nan values in exp_info\n",
    "   assert not exp_info.isnull().values.any()\n",
    "   exp_info = exp_info[exp_info['#fribbles'] == 1]\n",
    "   # Here, we assert that all detected shared expressions are in the original lemmatized utterances, with matching turn numbers\n",
    "   assert_match_betwee_shared_exp_and_actual_utterances(turns_info, shared_constructions_info)\n",
    "   return exp_info\n",
    "\n",
    "real_exp_info = clean_exp_info(real_exp_info, function_words, stop_words)\n",
    "pseudo_exp_info = clean_exp_info(pseudo_exp_info, function_words, stop_words)\n",
    "\n",
    "real_exp_info = link_exp_info(real_exp_info, real_turns_info, real_shared_constructions_info)\n",
    "pseudo_exp_info = link_exp_info(pseudo_exp_info, pseudo_turns_info, pseudo_shared_constructions_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "445d7611",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4. Linking shared constructions with a common lexical core <a class=\"anchor\" id=\"linking-shared-constructions\"></a>\n",
    "\n",
    "The methods described above may result in sets of unique shared expressions per fribble that contain highly overlapping expressions. For example, for fribble 14, pair 10 uses the shared expressions \"de bal bovenop\" and \"de bal,\" which overlap significantly. To identify such overlaps, we use regular expressions and link the expressions based on the overlap of content lemmas. We do not link the expressions if the overlapping sequence contains only function words.\n",
    "\n",
    "This process produces sets of shared expression types that reflect a common underlying conceptualization or lexical core, which we consider instances of lexical alignment. To simplify the labeling of these types, we assign a common lexical core as the label, which is the shortest common sequence for the set. For instance, Pair 5 has three sets of shared expressions, each labeled with a common lexical core, such as \"paddestoel,\" which includes three shared expressions: \"een paddestoel,\" \"een paddestoel op zijn hoofd,\" and \"paddestoel op zijn hoofd.\"\n",
    "\n",
    "To illustrate the linking process of shared expressions, consider the example below for Pair 5 and Fribble 1.\n",
    "<!-- > **PLOT** include plot for Pair 5 example after linking; for each occurance include which speaker (A,B) utters it. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9826cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.link_target_expressions import final_linking_algo_individually\n",
    "real_fribble_specific_exp_info = final_linking_algo_individually(real_turns_info, real_exp_info, real_shared_constructions_info, function_words, pos_func_words, pseudo_pairs=False, frequent_words=frequent_words)\n",
    "pseudo_fribble_specific_exp_info = final_linking_algo_individually(pseudo_turns_info, pseudo_exp_info, pseudo_shared_constructions_info, function_words, pos_func_words, pseudo_pairs=True, frequent_words=frequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ea13e",
   "metadata": {},
   "source": [
    "## Example <a class=\"anchor\" id=\"example\"></a>\n",
    "To illustrate the linking process of shared expressions, consider the example below for Pair 5 and Fribble 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fribbles = 1\n",
    "target_fribble = '01'\n",
    "target_pair = 'pair05'\n",
    "pair = target_pair\n",
    "real_exp_info['number of rounds'] = real_exp_info.apply(lambda x: len(np.unique(x['rounds'])), axis=1)\n",
    "real_exp_info['label'] = real_exp_info['exp']\n",
    "linked_expressions = link_shared_expressions_fribble_based(real_turns_info, real_exp_info, function_words, pos_func_words, num_fribbles=num_fribbles, target_fribble=target_fribble, target_pair=target_pair, fribbles_path=fribbles_path, videos_path=videos_path, save_clips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_fribble_specific_exp_info['Pairs'] = 'Pseudo'\n",
    "real_fribble_specific_exp_info['Pairs'] = 'Real'\n",
    "# combine the pseudo and real data\n",
    "fribble_specific_exp_info = pd.concat([real_fribble_specific_exp_info, pseudo_fribble_specific_exp_info])\n",
    "# combine turns_info and pseudo_turns_info dictionaries\n",
    "turns_info = {**real_turns_info, **pseudo_turns_info}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f50cc",
   "metadata": {},
   "source": [
    "# Now, we have two main Dataframes:\n",
    "* fribble_specific_exp_info: where each row corresponds to a shared construction type. The other columns show the types of features (such as rounds, turns, etc). It contains the construction types for real and pseudo pairs.\n",
    "\n",
    "* real_exp_info: Each row contains a shared construction for real pairs\n",
    "* pseudo_exp_info: Each row contains a shared construction for pseudo pairs\n",
    "\n",
    "Check the example below for fribble_specific_exp_info where shared expressions --> shared constructions , where label is the common lexical item for the shared constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78256f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fribble_specific_exp_info.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cecb18",
   "metadata": {},
   "source": [
    "# Prepare the data of the Naming Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gather_naming_similarities_pseudo import get_pre_post_pseudo_names\n",
    "from utils.gather_naming_similarities import get_pre_post_names, calculate_features_for_each_fribble_per_pair, measure_distances_between_names\n",
    "from utils.calculate_shared_constructions_features import calculate_main_simple_feats\n",
    "\n",
    "def get_names_for_pseudo_pairs(fribble_specific_exp_info, naming_task):\n",
    "   pre_post_pseudo_names = get_pre_post_pseudo_names(naming_task, fribble_specific_exp_info)\n",
    "   return pre_post_pseudo_names\n",
    "\n",
    "pre_post_real_names = get_pre_post_names(naming_task, fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Real'])\n",
    "pre_post_real_names = measure_distances_between_names(pre_post_real_names, fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Real'])\n",
    "pre_post_real_names = calculate_main_simple_feats(pre_post_real_names, fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Real'])\n",
    "pre_post_real_names = calculate_features_for_each_fribble_per_pair(pre_post_real_names, fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Real'])\n",
    "\n",
    "pre_post_names_pseudo = get_names_for_pseudo_pairs(fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Pseudo'], naming_task)\n",
    "pre_post_names_pseudo = measure_distances_between_names(pre_post_names_pseudo, fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Pseudo'])\n",
    "pre_post_names_pseudo = calculate_main_simple_feats(pre_post_names_pseudo, fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Pseudo'])\n",
    "pre_post_names_pseudo = calculate_features_for_each_fribble_per_pair(pre_post_names_pseudo, fribble_specific_exp_info[fribble_specific_exp_info['Pairs'] == 'Pseudo'])\n",
    "\n",
    "pre_post_names_pseudo['Pairs'] = 'Pseudo'\n",
    "pre_post_real_names['Pairs'] = 'Real'\n",
    "pre_post_names = pd.concat([pre_post_names_pseudo, pre_post_real_names])\n",
    "pre_post_names = pre_post_names.rename(columns={'number of core labels': 'Number of shared expression types'})\n",
    "pre_post_names['Number of shared expression types'] = pre_post_names['Number of shared expression types'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f213fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_fribble_specific_exp_info['Pairs'] = 'Pseudo'\n",
    "real_fribble_specific_exp_info['Pairs'] = 'Real'\n",
    "# combine the pseudo and real data\n",
    "fribble_specific_exp_info = pd.concat([real_fribble_specific_exp_info, pseudo_fribble_specific_exp_info])\n",
    "# combine turns_info and pseudo_turns_info dictionaries\n",
    "turns_info = {**real_turns_info, **pseudo_turns_info}\n",
    "real_fribble_specific_exp_info['target_fribble'] = real_fribble_specific_exp_info['target_fribbles'].apply(lambda x: x[0])\n",
    "pseudo_fribble_specific_exp_info['target_fribble'] = pseudo_fribble_specific_exp_info['target_fribbles'].apply(lambda x: x[0]) \n",
    "len_exp_per_fribble_and_pair_real = real_fribble_specific_exp_info['length'].to_numpy()\n",
    "len_exp_per_fribble_and_pair_psuedo = pseudo_fribble_specific_exp_info['length'].to_numpy()\n",
    "\n",
    "from utils.similarity_measures import prepare_per_speaker_for_all_data\n",
    "real_exp_info['int_pair'] = real_exp_info['pair'].apply(lambda x: int(x.replace('pair', '')))\n",
    "real_exp_info['Pairs'] = 'Real'\n",
    "\n",
    "pseudo_exp_info['pair'].apply(lambda x: x.replace('pair', '').replace('_and_', '')).astype(int)\n",
    "pseudo_exp_info['speaker_A_pair'] = pseudo_exp_info['pair'].apply(lambda x: int(x.replace('pair', '').split('_')[0]))\n",
    "pseudo_exp_info['speaker_B_pair'] = pseudo_exp_info['pair'].apply(lambda x: int(x.replace('pair', '').split('_')[-1]))\n",
    "# put the number next to each other\n",
    "pseudo_exp_info['int_pair'] = pseudo_exp_info['speaker_A_pair'].astype(str) + pseudo_exp_info['speaker_B_pair'].astype(str)\n",
    "pseudo_exp_info['int_pair'] = pseudo_exp_info['int_pair'].astype(int)\n",
    "# remove the speaker_A_pair and speaker_B_pair columns\n",
    "pseudo_exp_info = pseudo_exp_info.drop(columns=['speaker_A_pair', 'speaker_B_pair'])\n",
    "pseudo_exp_info['Pairs'] = 'Pseudo'\n",
    "pseudo_fribble_specific_exp_info['exp'] = pseudo_fribble_specific_exp_info['label']\n",
    "real_fribble_specific_exp_info['exp'] = real_fribble_specific_exp_info['label']\n",
    "pseudo_exp_info['target_fribble'] = pseudo_exp_info['target_fribbles']\n",
    "real_exp_info['target_fribble'] = real_exp_info['target_fribbles']\n",
    "sudo_speaker_turn_round = prepare_per_speaker_for_all_data(pre_post_names, pseudo_exp_info, pseudo_turns_info)\n",
    "real_speaker_turn_round = prepare_per_speaker_for_all_data(pre_post_names, real_exp_info, real_turns_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_fribble_specific_exp_info['target_fribble'] = real_fribble_specific_exp_info['target_fribbles'].apply(lambda x: x[0])\n",
    "pseudo_fribble_specific_exp_info['target_fribble'] = pseudo_fribble_specific_exp_info['target_fribbles'].apply(lambda x: x[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d68d0b8a",
   "metadata": {},
   "source": [
    "# 3. Analysis & Results <a class=\"anchor\" id=\"results\"></a> \n",
    "\n",
    "This section details the three analysis presented in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8280624",
   "metadata": {},
   "source": [
    "## 3.1. Presence of Shared Constructions and their Patterns of Use over Dialogue Rounds <a class=\"anchor\" id=\"presence_alignment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d651870",
   "metadata": {},
   "source": [
    "#### The percentage of shared constructions over rounds in dialogues vs. pseudo-dialogues\n",
    "An average of 34% of all utterances per dialogue include shared constructions. Figure below shows that this rate increases as the interaction progresses: from 27% in the first round to 37% of utterances containing shared constructions in the final round (Spearman’s rho = 0.36, p  << 0.001). In contrast, only 14% of utterances contain shared constructions in the pseudo-dialogues, with no increase over rounds. This indicates that alignment is largely the result of interaction, rather than solely the consequence of all dyads referring to the same objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis1 import similarity_of_names_with_utterances, plot_percentage_of_shared_constructions_over_rounds\n",
    "real_turns_utterances_names = similarity_of_names_with_utterances(real_turns_info, real_speaker_turn_round, pre_post_names, pseudo_or_real='Real', function_words=function_words, stop_words=stop_words, pos_func_words=frequent_words)\n",
    "pseudo_turns_utterances_names = similarity_of_names_with_utterances(pseudo_turns_info, sudo_speaker_turn_round, pre_post_names, pseudo_or_real='Pseudo', function_words=function_words, stop_words=stop_words, pos_func_words=frequent_words)\n",
    "plot_percentage_of_shared_constructions_over_rounds(real_turns_utterances_names, pseudo_turns_utterances_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db561f",
   "metadata": {},
   "source": [
    "## The number of shared constructions per fribble over rounds in dialogues vs. pseudo-dialogues\n",
    "Participants initially use several shared constructions types per referent and over time some of these constructions tend to be dropped, as common ground is built up. Note that, here we report the number of shared constructions, the reported numbers in the paper are for the number of shared construction types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_speaker_turn_round['rounds']\n",
    "real_rounds_numbers = real_speaker_turn_round.groupby(['pair', 'rounds', 'target_fribble']).size().reset_index(name='counts')\n",
    "real_rounds_numbers = real_rounds_numbers.rename(columns={'counts': 'Number of shared expressions'})\n",
    "real_rounds_numbers['Number of shared expressions'] = real_rounds_numbers['Number of shared expressions'].astype(int)\n",
    "# plot bar plot the number of shared expressions over rounds\n",
    "sns.set_theme(style=\"whitegrid\", palette=palette)\n",
    "sns.barplot(x='rounds', y='Number of shared expressions', data=real_rounds_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630e082",
   "metadata": {},
   "source": [
    "## 3.2. Individual Speaker Names versus Interactive Shared Constructions <a class=\"anchor\" id=\"individual_vs_shared\"></a>\n",
    "\n",
    "In our second analysis, we investigate the relation between cross-speaker alignment in the interactive task, as captured\n",
    "by shared constructions, and the names given to the fribbles by each participant in the individual naming task before and after the interaction. This analysis provides key information about what pairs bring to the table before the interaction and whether shared constructions established in interaction are linked to referential labels used after the dialogue.\n",
    "\n",
    "We start by examining whether the way participants individually name the fribbles is altered after the interactive task. For this, we calculate cosine similarity between the pre- and post-interaction names a participant gave for each fribble. If a speaker used the same name before and after the dialogue, we would obtain a cosine similarity of 1. Instead, we find that the average cosine similarity is 0.27 (std= 0.24), indicating that participants tend to name fribbles differently after the communicative task.\n",
    "\n",
    "To investigate to what extent this difference is mediated by interaction-based linguistic alignment, we compute the cosine similarity of the dyads’ shared construction types for fribble with the pre-and post-interaction names of each par- ticipant. On average, 41.3% ± 0.09 of pre-interaction names and 61.5% ± 0.10 of post-interaction names per participant overlap with the shared constructions. As shown in Figure below, we find that shared constructions are more similar to post than to pre-interaction names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis2 import plot_similarity_of_shared_constructions_to_pre_and_post_names\n",
    "plot_similarity_of_shared_constructions_to_pre_and_post_names(real_speaker_turn_round, real_turns_utterances_names, pseudo_turns_utterances_names, Pairs='Real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffff4f",
   "metadata": {},
   "source": [
    "## 3.3. Naming Convergence Across Speakers is Linked to Shared Constructions <a class=\"anchor\" id=\"naming_convergence\"></a>\n",
    "Finally, we investigate how similar the names individually given by each speaker to the fribbles before and after the interaction are to each other. Previous studies already showed that the level of cross-speaker name similarity increases after the communicative task. \n",
    "Here we ask: To what extent is this converging trend related to the patterns of use of shared constructions?\n",
    "\n",
    "We first compute the cosine similarity between the two pre-interaction names (Spre) and between the two post-interaction names (Spost ) given by the participants and then measure the difference between these two similarities (Spost − Spre). For actual participant pairs, we find that the mean cosine similarity between the two pre-interaction names is 0.06, while after the interaction, the participant names have a mean cosine similarity of 0.43. Thus, there is an average increase of 0.37 cosine similarity. In contrast, for the pseudo-pairs of participants, the average cosine similarity between the two post-interaction names is -0.07, and the similarity difference between pre-and post-interaction names is 0 in this pseudo-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 2)\n",
    "palette = sns.color_palette(cc.glasbey_hv, n_colors=25)\n",
    "sns.set_theme(style=\"whitegrid\", palette=palette) \n",
    "from utils.gather_naming_similarities import plot_real_vs_pseudo_pairs_scores\n",
    "plot_real_vs_pseudo_pairs_scores(pre_post_names, 'post_lemmas_lexical_similarity', 'Post lemmas similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c6e1f",
   "metadata": {},
   "source": [
    "## The correlation between the number of shared construction types and the cosine similarity of the post-interaction names\n",
    "\n",
    "Recall that our first analysis revealed that speakers may not immediately converge on a unique, simple shared construction type. Instead, they may use several complementary descriptions and entertain different alternative views of a referent, as illustrated by the example in Figure 2. We hypothesize that using many different shared construction types may be indicative of difficulty building common ground and finding a simple way to refer to an object, which could lead to less similar post-interaction names. Indeed, we find a weak negative correlation between the number of shared construction types and the cosine similarity of the post-interaction names (Spearman’s ρ = −0.13, p ≪ 0.001): the more construction types for a fribble, the less similar the post-interaction names tend to be, as shown in Figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis3 import plot_number_of_shared_constructions_and_names_similarity\n",
    "plot_number_of_shared_constructions_and_names_similarity(pre_post_names[pre_post_names['Pairs']=='Real'], 'Number of shared expression types', 'post_lemmas_lexical_similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a01c7",
   "metadata": {},
   "source": [
    "We also observe a recency and frequency effect as reported in the paper. Please see the figures shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a326de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis3 import plot_two_measures_vs_feature\n",
    "\n",
    "plot_two_measures_vs_feature(pre_post_names, 'Number of Rounds', 'post_lemmas_lexical_similarity', 'Increase in Lemmas Similarity (Cosine Distance)')\n",
    "pre_post_names.rename(columns={'number of core labels': 'Number of shared expression types'}, inplace=True)\n",
    "\n",
    "plot_two_measures_vs_feature(pre_post_names, 'Number of shared expression types', 'post_lemmas_lexical_similarity', 'Increase in Lemmas Similarity (Cosine Distance)')\n",
    "plot_two_measures_vs_feature(pre_post_names, 'Last Round', 'post_lemmas_lexical_similarity', 'Increase in Lemmas Similarity (Cosine Distance)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
